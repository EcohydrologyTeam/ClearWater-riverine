{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e46dbb7",
   "metadata": {},
   "source": [
    "# Example of Reading an EFDC Output Binary File\n",
    "\n",
    "This notebook provides an example of reading an EFDC output binary file \"DYECON.bin\". The python module \"binary_reader.py\" from LimnoTech's proprietary Python package \"LTPy\" was used in this example. When the \"EFDCBinaryReader\" object is initialize it requires a csv file named \"bin_config.csv\" to be in a directory named \"data\" if no directory for the config file is provided. The \"bin_config.csv\" file has been placed in the \"data\" directory for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c940c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from binary_reader import EFDCBinaryReader\n",
    "from dask.dataframe import from_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00547a78",
   "metadata": {},
   "source": [
    "## EFDC Model Files Names\n",
    "The name of the EFDC main input deck needs to be provided. In almost all cases this will be \"efdc.inp\". The name of the EFDC output binary file type that will be read also needs to be provided. In this example the dye concentration file \"DYECON.bin\" will be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0fd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFDC_INP_FILE = '\\\\\\\\efdc.inp'\n",
    "EFDC_DYE_FILE = '\\\\\\\\DYECON.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f45d0",
   "metadata": {},
   "source": [
    "## Additional EFDC Simulation Specific Items\n",
    "The EFDC model files do not store an actual date and time of when the simulation begins; the model zero date/time (data_begin) is user defined seperate of any EFDC input files. As a side note LimnoTech's proprietary post-processing software \"WinModel\" does have an associated database where zero date/time is stored, but this \"*_EFDC.mdb\" database may not be present for a given EFDC simulation directory. In addition to date_begin, the EFDC simulation directory (efdc_root) will also need to be provided.\n",
    "\n",
    "The DYECON.bin file for this example is over 100 MB (along with several other output files from this simulation) which exceeds the github upload limit. These EFDC binary output files will be excluded from github but they may be found on the LimnoTech network at: I:\\2ERDC12\\Task5_ClearWater\\EFDC_Example\\20110125_2007Baseline_1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19641ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_begin = datetime(1991,12,20)\n",
    "efdc_root = r'C:\\\\Users\\\\jrutyna\\\\githubDesktop\\\\ClearWater-riverine\\\\examples\\\\Read_EFDC_Example\\\\20110125_2007Baseline_1992'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48662a",
   "metadata": {},
   "source": [
    "## Use EFDC Binary Reader\n",
    "A pandas dataframe will be created when the EFDC binary reader is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89532fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file {0} DYECON.bin\n",
      "Field name: DYE_mgL\n",
      "Assumed binary file units: mg/L\n",
      "Output units: mg/L\n",
      "Applying conversion factor: 1 mg/L = 1 mg/L\n"
     ]
    }
   ],
   "source": [
    "reader = EFDCBinaryReader(date_begin=date_begin, path_efdcinp=efdc_root + EFDC_INP_FILE)\n",
    "df_dye = reader.process_bin_file(efdc_root + EFDC_DYE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca7a01",
   "metadata": {},
   "source": [
    "## Check if the dataframe was created\n",
    "The reader will provide the model results by EFDC \"grid_no\" and then by all of the output timesteps corrected to an actual data/time format based on \"date_begin\". These results can be mapped by \"grid_no\" with the provided \"EFDC.shp\" shape file in the \"shapefiles\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578e9d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           grid_no            datetime  DYE_mgL\n",
       "0               1 1991-12-20 00:00:00      0.0\n",
       "1               1 1991-12-20 01:00:00      0.0\n",
       "2               1 1991-12-20 02:00:00      0.0\n",
       "3               1 1991-12-20 03:00:00      0.0\n",
       "4               1 1991-12-20 04:00:00      0.0\n",
       "...           ...                 ...      ...\n",
       "28023403     3089 1992-12-31 19:00:00      0.0\n",
       "28023404     3089 1992-12-31 20:00:00      0.0\n",
       "28023405     3089 1992-12-31 21:00:00      0.0\n",
       "28023406     3089 1992-12-31 22:00:00      0.0\n",
       "28023407     3089 1992-12-31 23:00:00      0.0\n",
       "\n",
       "[28023408 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dye.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67adda3",
   "metadata": {},
   "source": [
    "## Export pandas dataframe as parquet data files\n",
    "Since the DYECON.bin file exceeded github upload limits the pandas dataframe will be exported as several parquet data files. There are over 3,000 EFDC model cells in this example and the parquet partitions will be created by rounding the EFDC grid number to the nearest thousand. The pandas dataframe will be converted to a dask dataframe since the dask \"to_parquet\" method has a \"partition_on\" parameter that is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7a030f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _Frame.head of Dask DataFrame Structure:\n",
       "              grid_no        datetime  DYE_mgL CellGroup\n",
       "npartitions=1                                           \n",
       "0               int32  datetime64[ns]  float32     int32\n",
       "28023407          ...             ...      ...       ...\n",
       "Dask Name: from_pandas, 1 tasks>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dye['CellGroup'] = df_dye.grid_no.round(decimals=-3)\n",
    "dd_dye = from_pandas(df_dye, npartitions=1)\n",
    "dd_dye.to_parquet(\n",
    "   path='C:/Users/jrutyna/githubDesktop/ClearWater-riverine/examples/Read_EFDC_Example/daskExport',\n",
    "   engine='pyarrow',\n",
    "   compression='gzip',\n",
    "   partition_on=['CellGroup']\n",
    ")\n",
    "dd_dye.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
